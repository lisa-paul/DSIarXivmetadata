#import libraries

#read in 3G json

#make into DF (-s if lag etc)

#?? read out in csv??? this could cause the same lag as the 1k+ splitted files
#write out a file that's like half the size without half the fields???

#Use Streaming: Instead of reading the entire file into memory at once, consider using a streaming approach, where you process the file incrementally. This can be achieved using libraries like ijson in Python.
#per chatgpt

# pandas with the chunksize parameter. <- also per the same

#If your analysis involves filtering or manipulating the data, you can perform these operations on smaller subsets of the data to avoid loading the entire dataset into memory.

#therefore, what if I split it into like 3 1g files or so?
still better for reading but at least I wouldn't crash git haha

